{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk import conlltags2tree\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_conll_iob(annotated_sentence):\n",
    "    \"\"\"\n",
    "    `annotated_sentence` = list of triplets [(w1, t1, iob1), ...]\n",
    "    Transform a pseudo-IOB notation: O, PERSON, PERSON, O, O, LOCATION, O\n",
    "    to proper IOB notation: O, B-PERSON, I-PERSON, O, O, B-LOCATION, O\n",
    "    \"\"\"\n",
    "    proper_iob_tokens = []\n",
    "    for idx, annotated_token in enumerate(annotated_sentence):\n",
    "        tag, word, ner = annotated_token\n",
    " \n",
    "        if ner != 'O':\n",
    "            if idx == 0:\n",
    "                ner = \"B-\" + ner\n",
    "            elif annotated_sentence[idx - 1][2] == ner:\n",
    "                ner = \"I-\" + ner\n",
    "            else:\n",
    "                ner = \"B-\" + ner\n",
    "        proper_iob_tokens.append((tag, word, ner))\n",
    "    return proper_iob_tokens\n",
    " \n",
    "def read_gmb_ner(corpus_root):\n",
    "    for root, dirs, files in os.walk(corpus_root):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".tags\"):\n",
    "                with open(os.path.join(root, filename), 'rb') as file_handle:\n",
    "                    file_content = file_handle.read().decode('utf-8').strip()\n",
    "                    annotated_sentences = file_content.split('\\n\\n')\n",
    "                    for annotated_sentence in annotated_sentences:\n",
    "                        annotated_tokens = [seq for seq in annotated_sentence.split('\\n') if seq]\n",
    " \n",
    "                        standard_form_tokens = []\n",
    " \n",
    "                        for idx, annotated_token in enumerate(annotated_tokens):\n",
    "                            annotations = annotated_token.split('\\t')\n",
    "                            word, tag, ner = annotations[0], annotations[1], annotations[3]\n",
    " \n",
    "                            if ner != 'O':\n",
    "                                ner = ner.split('-')[0]\n",
    " \n",
    "                            standard_form_tokens.append((word, tag, ner))\n",
    " \n",
    "                        conll_tokens = to_conll_iob(standard_form_tokens)\n",
    "                        yield conlltags2tree(conll_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape(word):\n",
    "    word_shape = 'other'\n",
    "    if re.match('[0-9]+(\\.[0-9]*)?|[0-9]*\\.[0-9]+$', word):\n",
    "        word_shape = 'number'\n",
    "    elif re.match('\\W+$', word):\n",
    "        word_shape = 'punct'\n",
    "    elif re.match('[A-Z][a-z]+$', word):\n",
    "        word_shape = 'capitalized'\n",
    "    elif re.match('[A-Z]+$', word):\n",
    "        word_shape = 'uppercase'\n",
    "    elif re.match('[a-z]+$', word):\n",
    "        word_shape = 'lowercase'\n",
    "    elif re.match('[A-Z][a-z]+[A-Z][a-z]+[A-Za-z]*$', word):\n",
    "        word_shape = 'camelcase'\n",
    "    elif re.match('[A-Za-z]+$', word):\n",
    "        word_shape = 'mixedcase'\n",
    "    elif re.match('__.+__$', word):\n",
    "        word_shape = 'wildcard'\n",
    "    elif re.match('[A-Za-z0-9]+\\.$', word):\n",
    "        word_shape = 'ending-dot'\n",
    "    elif re.match('[A-Za-z0-9]+\\.[A-Za-z0-9\\.]+\\.$', word):\n",
    "        word_shape = 'abbreviation'\n",
    "    elif re.match('[A-Za-z0-9]+\\-[A-Za-z0-9\\-]+.*$', word):\n",
    "        word_shape = 'contains-hyphen'\n",
    " \n",
    "    return word_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    " \n",
    "def ner_features(tokens, index, history):\n",
    "    \"\"\"\n",
    "    `tokens`  = a POS-tagged sentence [(w1, t1), ...]\n",
    "    `index`   = the index of the token we want to extract features for\n",
    "    `history` = the previous predicted IOB tags\n",
    "    \"\"\"\n",
    " \n",
    "    # Pad the sequence with placeholders\n",
    "    tokens = [('__START2__', '__START2__'), ('__START1__', '__START1__')] + list(tokens) + [('__END1__', '__END1__'), ('__END2__', '__END2__')]\n",
    "    history = ['__START2__', '__START1__'] + list(history)\n",
    " \n",
    "    # shift the index with 2, to accommodate the padding\n",
    "    index += 2\n",
    " \n",
    "    word, pos = tokens[index]\n",
    "    prevword, prevpos = tokens[index - 1]\n",
    "    prevprevword, prevprevpos = tokens[index - 2]\n",
    "    nextword, nextpos = tokens[index + 1]\n",
    "    nextnextword, nextnextpos = tokens[index + 2]\n",
    "    previob = history[-1]\n",
    "    prevpreviob = history[-2]\n",
    " \n",
    "    feat_dict = {\n",
    "        'word': word,\n",
    "        'lemma': stemmer.stem(word),\n",
    "        'pos': pos,\n",
    "        'shape': shape(word),\n",
    " \n",
    "        'next-word': nextword,\n",
    "        'next-pos': nextpos,\n",
    "        'next-lemma': stemmer.stem(nextword),\n",
    "        'next-shape': shape(nextword),\n",
    " \n",
    "        'next-next-word': nextnextword,\n",
    "        'next-next-pos': nextnextpos,\n",
    "        'next-next-lemma': stemmer.stem(nextnextword),\n",
    "        'next-next-shape': shape(nextnextword),\n",
    " \n",
    "        'prev-word': prevword,\n",
    "        'prev-pos': prevpos,\n",
    "        'prev-lemma': stemmer.stem(prevword),\n",
    "        'prev-iob': previob,\n",
    "        'prev-shape': shape(prevword),\n",
    " \n",
    "        'prev-prev-word': prevprevword,\n",
    "        'prev-prev-pos': prevprevpos,\n",
    "        'prev-prev-lemma': stemmer.stem(prevprevword),\n",
    "        'prev-prev-iob': prevpreviob,\n",
    "        'prev-prev-shape': shape(prevprevword),\n",
    "    }\n",
    " \n",
    "    return feat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    " \n",
    "from nltk import tree2conlltags\n",
    "from nltk.chunk import ChunkParserI\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ScikitLearnChunker(ChunkParserI):\n",
    " \n",
    "    @classmethod\n",
    "    def to_dataset(cls, parsed_sentences, feature_detector):\n",
    "        \"\"\"\n",
    "        Transform a list of tagged sentences into a scikit-learn compatible POS dataset\n",
    "        :param parsed_sentences:\n",
    "        :param feature_detector:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        for parsed in parsed_sentences:\n",
    "            iob_tagged = tree2conlltags(parsed)\n",
    "            words, tags, iob_tags = zip(*iob_tagged)\n",
    " \n",
    "            tagged = zip(words, tags)\n",
    " \n",
    "            for index in range(len(iob_tagged)):\n",
    "                X.append(feature_detector(tagged, index, history=iob_tags[:index]))\n",
    "                y.append(iob_tags[index])\n",
    " \n",
    "        return X, y\n",
    " \n",
    "    @classmethod\n",
    "    def get_minibatch(cls, parsed_sentences, feature_detector, batch_size=500):\n",
    "        batch = list(itertools.islice(parsed_sentences, batch_size))\n",
    "        X, y = cls.to_dataset(batch, feature_detector)\n",
    "        return X, y\n",
    " \n",
    "    @classmethod\n",
    "    def train(cls, parsed_sentences, feature_detector, all_classes, **kwargs):\n",
    "        X, y = cls.get_minibatch(parsed_sentences, feature_detector, kwargs.get('batch_size', 500))\n",
    "        vectorizer = DictVectorizer(sparse=False)\n",
    "        vectorizer.fit(X)\n",
    " \n",
    "        clf = Perceptron(verbose=10, n_jobs=-1, n_iter=kwargs.get('n_iter', 5))\n",
    " \n",
    "        while len(X):\n",
    "            X = vectorizer.transform(X)\n",
    "            clf.partial_fit(X, y, all_classes)\n",
    "            X, y = cls.get_minibatch(parsed_sentences, feature_detector, kwargs.get('batch_size', 500))\n",
    " \n",
    "        clf = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    " \n",
    "        return cls(clf, feature_detector)\n",
    " \n",
    "    def __init__(self, classifier, feature_detector):\n",
    "        self._classifier = classifier\n",
    "        self._feature_detector = feature_detector\n",
    " \n",
    "    def parse(self, tokens):\n",
    "        \"\"\"\n",
    "        Chunk a tagged sentence\n",
    "        :param tokens: List of words [(w1, t1), (w2, t2), ...]\n",
    "        :return: chunked sentence: nltk.Tree\n",
    "        \"\"\"\n",
    "        history = []\n",
    "        iob_tagged_tokens = []\n",
    "        for index, (word, tag) in enumerate(tokens):\n",
    "            iob_tag = self._classifier.predict([self._feature_detector(tokens, index, history)])[0]\n",
    "            history.append(iob_tag)\n",
    "            iob_tagged_tokens.append((word, tag, iob_tag))\n",
    " \n",
    "        return conlltags2tree(iob_tagged_tokens)\n",
    " \n",
    "    def score(self, parsed_sentences):\n",
    "        \"\"\"\n",
    "        Compute the accuracy of the tagger for a list of test sentences\n",
    "        :param parsed_sentences: List of parsed sentences: nltk.Tree\n",
    "        :return: float 0.0 - 1.0\n",
    "        \"\"\"\n",
    "        X_test, y_test = self.__class__.to_dataset(parsed_sentences, self._feature_detector)\n",
    "        return self._classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_perceptron():\n",
    "    reader = read_gmb_ner(\"gmb-2.2.0\")\n",
    " \n",
    "    all_classes = ['O', 'B-per', 'I-per', 'B-gpe', 'I-gpe', \n",
    "                   'B-geo', 'I-geo', 'B-org', 'I-org', 'B-tim', 'I-tim',\n",
    "                   'B-art', 'I-art', 'B-eve', 'I-eve', 'B-nat', 'I-nat']\n",
    " \n",
    "    pa_ner = ScikitLearnChunker.train(itertools.islice(reader, 50000), feature_detector=ner_features,\n",
    "                                                   all_classes=all_classes, batch_size=500, n_iter=5)\n",
    "    accuracy = pa_ner.score(itertools.islice(reader, 5000))\n",
    "    print (\"Accuracy:\", accuracy) # 0.970327096314"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
